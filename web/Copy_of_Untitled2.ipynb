{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Suh-KcJkmrPJ",
        "outputId": "242d1bea-0f8a-4091-94ac-69eba81e2851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "T-KWG5_ZnE23",
        "outputId": "1f68855e-8b15-42e3-f738-0bf6a29e3620"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-47a756f6dc1b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Unzip the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… Unzipped files to: {extract_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1756\u001b[0;31m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1757\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to your uploaded ZIP file in Colab\n",
        "zip_path = \"/content/drive/MyDrive/LinearProject/AAAZ.zip\"\n",
        "extract_dir = \"/content/drive/MyDrive/LinearProject\"\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"âœ… Unzipped files to: {extract_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8DDWDgWnLJB",
        "outputId": "b42c20a1-8149-41e1-d836-a522318ca23b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [13:49<00:00, 13.59s/it]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "vggface2_path = \"/content/drive/MyDrive/LinearProject\"  # Replace with your dataset path\n",
        "MAX_PEOPLE = 60#####\n",
        "MAX_IMAGES_PER_PERSON = 500 ######\n",
        "IMAGE_SIZE = (128, 128)\n",
        "\n",
        "# ============================\n",
        "# ðŸ“¦ Load VGGFace2 Images\n",
        "# ============================\n",
        "def load_vggface2_images(path, image_size=(128, 128)):\n",
        "    images, labels = [], []\n",
        "    label_map = {}\n",
        "    current_label = 0\n",
        "\n",
        "    for celeb_folder in tqdm(sorted(os.listdir(path)), desc=\"Loading\"):\n",
        "        if current_label >= MAX_PEOPLE:\n",
        "            break\n",
        "        celeb_path = os.path.join(path, celeb_folder)\n",
        "        if not os.path.isdir(celeb_path):\n",
        "            continue\n",
        "        label_map[current_label] = celeb_folder\n",
        "        count = 0\n",
        "        for img_file in os.listdir(celeb_path):\n",
        "            if count >= MAX_IMAGES_PER_PERSON:\n",
        "                break\n",
        "            img_path = os.path.join(celeb_path, img_file)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "            try:\n",
        "                img = cv2.resize(img, image_size)\n",
        "                img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                img_flat = img_gray.flatten() / 255.0  # Normalize\n",
        "                images.append(img_flat)\n",
        "                labels.append(current_label)\n",
        "                count += 1\n",
        "            except:\n",
        "                continue\n",
        "        current_label += 1\n",
        "\n",
        "    return np.array(images), np.array(labels), label_map\n",
        "\n",
        "# Load data\n",
        "X, y, label_map = load_vggface2_images(vggface2_path, IMAGE_SIZE)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=50)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# SVM with grid search\n",
        "svm = GridSearchCV(SVC(), {'C': [1, 10], 'kernel': ['linear', 'rbf']}, cv=3, n_jobs=-1)\n",
        "svm.fit(X_train_pca, y_train)\n",
        "best_model = svm.best_estimator_\n",
        "\n",
        "# Predict\n",
        "y_pred = best_model.predict(X_test_pca)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nâœ… Evaluation:\")\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "\n",
        "# Label names\n",
        "class_names = [label_map[i] for i in sorted(label_map)]\n",
        "\n",
        "# Report\n",
        "print(\"\\nðŸ“‹ Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# ============================\n",
        "# ðŸ§  Predict & Visualize Random Test Image\n",
        "# ============================\n",
        "def visualize_prediction(index=0):\n",
        "    img_pca = X_test_pca[index].reshape(1, -1)\n",
        "    pred_label = best_model.predict(img_pca)[0]\n",
        "    true_label = y_test[index]\n",
        "    pred_name = label_map[pred_label]\n",
        "    true_name = label_map[true_label]\n",
        "\n",
        "    original_flat = pca.inverse_transform(img_pca).reshape(IMAGE_SIZE)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(original_flat, cmap='gray')\n",
        "    plt.title(f\"Predicted: {pred_name} | True: {true_name}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Show prediction on first test image\n",
        "visualize_prediction(index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKNVvImvp8tJ",
        "outputId": "e2523bb9-ae3d-4455-8327-3bb8126aa465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 24/61 [1:17:54<2:04:53, 202.52s/it]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression  # You can swap with SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Configuration\n",
        "vggface2_path = \"/content/drive/MyDrive/LinearProject\"  # Replace with your dataset path\n",
        "MAX_PEOPLE = 60\n",
        "MAX_IMAGES_PER_PERSON = 500\n",
        "IMAGE_SIZE = (224, 224)  # ResNet requires 224x224 input\n",
        "\n",
        "# Load ResNet50 model (pretrained)\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')  # Output shape: (2048,)\n",
        "\n",
        "# =======================================\n",
        "# ðŸ“† Load Images and Extract ResNet Features\n",
        "# =======================================\n",
        "def extract_resnet_features(path, image_size=(224, 224)):\n",
        "    features, labels = [], []\n",
        "    label_map = {}\n",
        "    current_label = 0\n",
        "\n",
        "    for celeb_folder in tqdm(sorted(os.listdir(path)), desc=\"Extracting\"):\n",
        "        if current_label >= MAX_PEOPLE:\n",
        "            break\n",
        "        celeb_path = os.path.join(path, celeb_folder)\n",
        "        if not os.path.isdir(celeb_path):\n",
        "            continue\n",
        "        label_map[current_label] = celeb_folder\n",
        "        count = 0\n",
        "        for img_file in os.listdir(celeb_path):\n",
        "            if count >= MAX_IMAGES_PER_PERSON:\n",
        "                break\n",
        "            img_path = os.path.join(celeb_path, img_file)\n",
        "            try:\n",
        "                img = image.load_img(img_path, target_size=image_size)\n",
        "                img_array = image.img_to_array(img)\n",
        "                img_array = np.expand_dims(img_array, axis=0)\n",
        "                img_array = preprocess_input(img_array)\n",
        "\n",
        "                feat = resnet.predict(img_array, verbose=0)\n",
        "                features.append(feat.flatten())\n",
        "                labels.append(current_label)\n",
        "                count += 1\n",
        "            except:\n",
        "                continue\n",
        "        current_label += 1\n",
        "\n",
        "    return np.array(features), np.array(labels), label_map\n",
        "\n",
        "# Load data\n",
        "X, y, label_map = extract_resnet_features(vggface2_path, IMAGE_SIZE)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=100)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Classifier\n",
        "clf = LogisticRegression(max_iter=1000)  # Replace with SVC if needed\n",
        "clf.fit(X_train_pca, y_train)\n",
        "y_pred = clf.predict(X_test_pca)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nâœ… Evaluation:\")\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score:  {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "\n",
        "# Label names\n",
        "class_names = [label_map[i] for i in sorted(label_map)]\n",
        "\n",
        "# Report\n",
        "print(\"\\nðŸ“‹ Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# ============================\n",
        "# ðŸ§  Predict & Visualize Random Test Image\n",
        "# ============================\n",
        "def visualize_prediction(index=0):\n",
        "    img_pca = X_test_pca[index].reshape(1, -1)\n",
        "    pred_label = clf.predict(img_pca)[0]\n",
        "    true_label = y_test[index]\n",
        "    pred_name = label_map[pred_label]\n",
        "    true_name = label_map[true_label]\n",
        "\n",
        "    # Inverse PCA + reshape (approximate visualization)\n",
        "    original_flat = pca.inverse_transform(img_pca).reshape((1, -1))\n",
        "    plt.figure(figsize=(6, 1))\n",
        "    plt.imshow(original_flat, aspect='auto', cmap='gray')\n",
        "    plt.title(f\"Predicted: {pred_name} | True: {true_name}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Show prediction on first test image\n",
        "visualize_prediction(index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "35R8WFfBs3YR",
        "outputId": "53e8d4d4-ab96-4f16-a763-c023ef6c6630"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Mountpoint must not already contain files",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}